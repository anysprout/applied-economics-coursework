{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group One - Presentation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************************  READ ME  *****************************\n",
    "\n",
    "The first code block in 'Data Preperation' needs to be updated. Update the 'input_path' variable to the directory where you unzipped this notebook's folder. \n",
    "\n",
    "For example yours might be something like: C:/users/username/downloads\n",
    "\n",
    "The raw data is stored within the ism645_flight_delay_analysis/raw_data folder. \n",
    "\n",
    "The 'raw_data' folder contains sub-folders with files for RDU flight data from the Bureau of Transportation Statistics (bts) and weather data for RDU from OpenWeather.\n",
    "\n",
    "******************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flight delays are a significant challenge for airport operations, passenger experience, and resource planning. This project combines 2023 historical flight data from the Bureau of Transportation Statistics (BTS) with local weather data from OpenWeather to analyze delay patterns at Raleigh-Durham International Airport (RDU). The objective is to uncover key factors that contribute to delays and develop actionable insights that can enhance operational efficiency and passenger expectations.\n",
    "\n",
    "### Dataset Overview\n",
    "The analysis uses two primary datasets:\n",
    "\n",
    "OpenWeather. History Bulk: Weather Data.\n",
    "\n",
    "https://openweathermap.org/history-bulk\n",
    "\n",
    "Bureau of Transportation Statistics. Airline On-Time Performance Data. U.S. Department of Transportation. \n",
    "\n",
    "https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ\n",
    "\n",
    "### Flight Data\n",
    "Sourced from the Bureau of Transportation Statistics (BTS), this dataset includes operational and delay-related information for 2023 flights. Key variables include:\n",
    "\n",
    "### Carrier Information:\n",
    "- `OP_UNIQUE_CARRIER`: Unique identifier for the operating carrier.\n",
    "\n",
    "### Route Details:\n",
    "- `ORIGIN`: Origin airport code.\n",
    "- `DEST`: Destination airport code.\n",
    "- `DISTANCE`: Flight distance in miles.\n",
    "\n",
    "### Flight Timing and Delays:\n",
    "- `DEP_TIME`: Actual departure time.\n",
    "- `DEP_DELAY_NEW`: Departure delay in minutes (only delays ≥ 0).\n",
    "- `DEP_DEL15`: Binary flag for 15+ minute departure delay (1 = yes, 0 = no).\n",
    "- `ARR_TIME`: Actual arrival time.\n",
    "- `ARR_DELAY_NEW`: Arrival delay in minutes (only delays ≥ 0).\n",
    "- `ARR_DEL15`: Binary flag for 15+ minute arrival delay (1 = yes, 0 = no).\n",
    "\n",
    "\n",
    "## Weather Data\n",
    "Sourced from OpenWeather, this dataset provides detailed meteorological conditions at RDU and surrounding regions, recorded at hourly intervals. Key variables include:\n",
    "\n",
    "### Temperature Metrics:\n",
    "- `temp`: Current temperature (°C).\n",
    "- `temp_min`: Minimum temperature during the period.\n",
    "- `temp_max`: Maximum temperature during the period.\n",
    "\n",
    "### Visibility:\n",
    "- `visibility`: Visibility range in meters.\n",
    "\n",
    "### Wind Conditions:\n",
    "- `wind_speed`: Average wind speed (m/s).\n",
    "- `wind_gust`: Maximum gust speed (m/s).\n",
    "\n",
    "### Precipitation:\n",
    "- `rain_1h`: Rain volume in the last hour (mm).\n",
    "- `rain_3h`: Rain volume in the last 3 hours (mm).\n",
    "- `snow_1h`: Snow volume in the last hour (mm).\n",
    "- `snow_3h`: Snow volume in the last 3 hours (mm).\n",
    "\n",
    "### Cloud Coverage:\n",
    "- `clouds_all`: Percentage of sky covered by clouds (0-100%).\n",
    "\n",
    "### Timestamp:\n",
    "- `dt_est`: Timestamp converted to Eastern Standard Time (EST).\n",
    "\n",
    "### Research Objectives\n",
    "The study aims to:\n",
    "- **Explore Patterns**: Examine how delays vary across time, carriers, and weather conditions.\n",
    "- **Perform Comparative Analysis**: Assess differences in delay frequencies by season, carrier, and time of day.\n",
    "- **Develop a Predictive Model**: Leverage flight and weather data to predict delay likelihood and magnitude.\n",
    "\n",
    "### Analytical Scope and Opportunities\n",
    "- **Temporal Analysis**: How do delays fluctuate across times of day and seasons?\n",
    "- **Carrier-Specific Trends**: Are certain airlines more prone to delays, and what operational factors may explain this?\n",
    "- **Weather Influence**: What is the relationship between weather variables (e.g., wind speed, precipitation) and delays?\n",
    "\n",
    "### Limitations\n",
    "This analysis has several limitations:\n",
    "- **Exclusion of Non-Weather Factors**: The focus is on weather-related data, with other potential delay factors, such as operational or logistical issues, left out.\n",
    "- **Year-Specific Data**: The dataset covers only 2023, which limits the ability to identify long-term trends or seasonal anomalies.\n",
    "- **Regional Specificity**: Findings are specific to RDU and may not apply to other airports or regions with different operational contexts.\n",
    "\n",
    "By combining detailed flight and weather data, this analysis aims to deepen our understanding of delay dynamics and provide actionable recommendations for mitigating delays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ******************************************************************************************\n",
    "# *      Update input_path to the directory where you extracted this notebook's folder     *\n",
    "# ******************************************************************************************\n",
    "#                                                                                          *\n",
    "input_path = \"C:/Projects\"                                                  # **************\n",
    "#                                                                                          *\n",
    "# ******************************************************************************************\n",
    "\n",
    "\n",
    "# list to hold temp dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Files are named 01 (for Jan), 02 (for Feb), and so on. The loop iterates through each file, filters for RDU data only, \n",
    "# appends data to the temporary list, and concatenates all data into the bts_df variable\n",
    "# Takes about 15 seconds depending on your device\n",
    "\n",
    "for i in range(12):\n",
    "    file_name = f\"ism645_flight_delay_analysis/raw_data/bts/{i+1:02}.csv\"\n",
    "    full_path = os.path.join(input_path, file_name)\n",
    "    \n",
    "    if i == 0:\n",
    "        # read in the first file and include the header row\n",
    "        df = pd.read_csv(full_path)\n",
    "    else:\n",
    "        # read in remaining files, skipping the header row\n",
    "        df = pd.read_csv(full_path, header=0)\n",
    "        \n",
    "    # filter for RDU flights only\n",
    "    rdu_flights = df[(df['ORIGIN'] == 'RDU') | (df['DEST'] == 'RDU')]\n",
    "    \n",
    "    dataframes.append(rdu_flights)\n",
    "\n",
    "# merge temp dataframes - all 2023 flights for RDU\n",
    "bts_df = pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataframe that contains data for all of 2023 specific to arrivals and departures from RDU, we can begin to create some additional variables and prepare the dataset for merger with the weather data. We'll start by adding labels to indicate the name of the airline based on the unique carrier ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the carrier codes to the common airline name\n",
    "airline_mapping = {\n",
    "    '9E': 'Endeavor Air',\n",
    "    'AA': 'American Airlines',\n",
    "    'AS': 'Alaska Airlines',\n",
    "    'B6': 'JetBlue Airways Corporation',\n",
    "    'DL': 'Delta Air Lines, Inc.',\n",
    "    'F9': 'Frontier Airlines, Inc.',\n",
    "    'G7': 'GoJet Airlines',\n",
    "    'MQ': 'Envoy Air',\n",
    "    'NK': 'Spirit Airlines, Inc.',\n",
    "    'OH': 'Jetstream Intl',\n",
    "    'OO': 'SkyWest Airlines',\n",
    "    'PT': 'Piedmont Airlines',\n",
    "    'UA': 'United Airlines, Inc.',\n",
    "    'WN': 'Southwest Airlines',\n",
    "    'YX': 'Republic Airlines',\n",
    "    'ZW': 'Air Wisconsin',\n",
    "    'YV': 'Mesa Airlines, Inc.'\n",
    "}\n",
    "\n",
    "# create a new column 'CARRIER_NAME' by mapping carrier codes to airline names\n",
    "bts_df['CARRIER_NAME'] = bts_df['OP_UNIQUE_CARRIER'].map(airline_mapping)\n",
    "\n",
    "### account for regional carriers that are subsidiaries of larger airlines ###\n",
    "# this section maps and adds a 'PARENT_ID' column representing the parent company of regional carriers\n",
    "bts_df['PARENT_ID'] = bts_df['OP_UNIQUE_CARRIER'].apply(lambda x: \n",
    "         'AA' if x in ['AA', 'OO', 'MQ', 'YV', 'PT']  # American Airlines subsidiaries\n",
    "    else 'DL' if x in ['DL', '9E', 'G7']              # Delta Airlines subsidiaries\n",
    "    else 'UA' if x in ['UA', 'ZW', 'OH']              # United Airlines subsidiaries\n",
    "    else x)                                           # for carriers without a parent, retain the original carrier code\n",
    "\n",
    "# adds the parent carrier name to the df based on PARENT_ID\n",
    "bts_df['PARENT_CARRIER_NAME'] = bts_df['PARENT_ID'].map(airline_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next step, we create a matchable datetime for each flight using both its flight date and arrival/departure time. The code ensures that the weather data corresponds to the appropriate time, filtered on if the flight is arriving or departing from RDU. The flight times are standardized and merged with the date to create a unified datetime format for comparison with the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extract a matchable datetime from FL_DATE and ARR/DEP_TIME ###\n",
    "\n",
    "# set arrival and departure times (flt) to Int\n",
    "bts_df['ARR_TIME'] = pd.to_numeric(bts_df['ARR_TIME'], errors='coerce').astype('Int64')\n",
    "bts_df['DEP_TIME'] = pd.to_numeric(bts_df['DEP_TIME'], errors='coerce').astype('Int64')\n",
    "\n",
    "# remove the unnecessary 12:00:00 AM timestamps from the FL_DATE column\n",
    "bts_df['FL_DATE'] = pd.to_datetime(bts_df['FL_DATE'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "bts_df['FL_DATE'] = bts_df['FL_DATE'].dt.date\n",
    "\n",
    "# set WEATHER_TIME to ARR_TIME if the destination is RDU. if the destination is not RDU, set WEATHER_TIME to DEP_TIME.\n",
    "bts_df['WEATHER_TIME'] = bts_df['ARR_TIME'].where(bts_df['DEST'] == 'RDU', bts_df['DEP_TIME'])\n",
    "\n",
    "# standardize WEATHER_TIME hour and minute (add leading 0 i.e. 0830)\n",
    "bts_df['WEATHER_TIME'] = bts_df['WEATHER_TIME'].astype(str).str.zfill(4)\n",
    "\n",
    "# extract the hour and minute data from WEATHER_TIME\n",
    "bts_df['WEATHER_HOUR'] = bts_df['WEATHER_TIME'].str[:2]\n",
    "bts_df['WEATHER_MINUTE'] = bts_df['WEATHER_TIME'].str[2:]\n",
    "\n",
    "# the corresponding hourly weather data (which is not resolved to the minute like the flight data)\n",
    "# combine FL_DATE, WEATHER_HOUR, and \"00\" for the minute value to create a datetime string with the hour floored\n",
    "# the flight times are floored to the hour to enable all flights within a given hour to be assigned\n",
    "bts_df['WEATHER_DATETIME'] = bts_df['FL_DATE'].astype(str) + ' ' + bts_df['WEATHER_HOUR'] + ':00'\n",
    "\n",
    "# convert the combined string to datetime\n",
    "bts_df['WEATHER_DATETIME'] = pd.to_datetime(bts_df['WEATHER_DATETIME'], errors='coerce')\n",
    "\n",
    "# drop any NaN WEATHER_DATETIME rows (these likely originate from lack of relevent ARR/DEP_TIME)\n",
    "bts_df = bts_df.dropna(subset=['WEATHER_DATETIME'])\n",
    "\n",
    "# sort the dataframe by WEATHER_DATETIME from earliest to latest date\n",
    "bts_df = bts_df.sort_values(by='WEATHER_DATETIME', ascending=True)\n",
    "\n",
    "# drop unnecessary columns from bts_df\n",
    "bts_df = bts_df[['OP_UNIQUE_CARRIER', 'ORIGIN', 'DEST', 'DEP_TIME',\n",
    "                       'DEP_DELAY_NEW', 'DEP_DEL15', 'ARR_TIME', 'ARR_DELAY_NEW', 'ARR_DEL15',\n",
    "                       'DISTANCE', 'CARRIER_NAME', 'PARENT_ID', 'PARENT_CARRIER_NAME', 'WEATHER_DATETIME']]\n",
    "\n",
    "# drop any duplicate rows\n",
    "bts_df = bts_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the weather data is loaded and cleaned. The dt_iso column is adjusted to remove timezone information and converted to a standard datetime format. We then convert the times from UTC to EST to align with the flight data. The relevant weather columns are retained for further analysis, and any duplicate or missing values in both the weather and flight datasets are removed to ensure data integrity before merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block also uses the input_path variable which needed to be updated. it will grab the raw data from within the open_weather folder\n",
    "\n",
    "raw_weather_data = f\"ism645_flight_delay_analysis/raw_data/open_weather/41e77af6ca4e61ed759f49fcee07b86f.csv\"\n",
    "weather_data_path = os.path.join(input_path, raw_weather_data)\n",
    "\n",
    "weather_df = pd.read_csv(weather_data_path)\n",
    "\n",
    "# remove the ' +0000 UTC'  info from dt_iso\n",
    "weather_df['dt_iso_clean'] = weather_df['dt_iso'].str.replace(r'\\s\\+\\d{4}\\sUTC$', '', regex=True)\n",
    "\n",
    "# convert dt_iso to datetime\n",
    "weather_df['dt'] = pd.to_datetime(weather_df['dt_iso_clean'], utc=True)\n",
    "\n",
    "# convert utc to est for dt_iso\n",
    "weather_df['dt_est'] = weather_df['dt'].dt.tz_convert('America/New_York')\n",
    "\n",
    "# drop duplicates\n",
    "weather_df = weather_df.drop_duplicates()\n",
    "\n",
    "# drop unneeded columns from weather_df\n",
    "weather_df = weather_df[['temp', \n",
    "                         'visibility', 'temp_min', 'temp_max', 'wind_speed', \n",
    "                         'wind_gust', 'rain_1h', 'rain_3h', 'snow_1h', 'snow_3h',\n",
    "                         'clouds_all', 'dt_est']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the data after removing the unneeded timezone information from the matched columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unneeded timezone information from both dataframes\n",
    "bts_df['WEATHER_DATETIME'] = pd.to_datetime(bts_df['WEATHER_DATETIME']).dt.tz_localize(None)\n",
    "weather_df['dt_est'] = pd.to_datetime(weather_df['dt_est']).dt.tz_localize(None)\n",
    "\n",
    "# merge on formatted datetime columns\n",
    "final_df = pd.merge(\n",
    "    bts_df,\n",
    "    weather_df,\n",
    "    left_on='WEATHER_DATETIME',\n",
    "    right_on='dt_est',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some aesthetic and clarity adjustments to column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename 'WEATHER_DATETIME' to 'DEP_ARR_TIME'\n",
    "final_df = final_df.rename(columns={'WEATHER_DATETIME': 'DEP_ARR_TIME'})\n",
    "\n",
    "# convert column names to lowercase because i think it looks nicer\n",
    "final_df.columns = final_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a categorical 'Season' variable to allow for delay analysis by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps season ranges for 2023 (including Winter from Jan 1 - Mar 19)\n",
    "season_ranges = {\n",
    "    'Winter1': ['2023-01-01', '2023-03-20'],\n",
    "    'Spring': ['2023-03-20', '2023-06-21'],\n",
    "    'Summer': ['2023-06-21', '2023-09-23'],\n",
    "    'Autumn': ['2023-09-23', '2023-12-21'],\n",
    "    'Winter2': ['2023-12-21', '2024-01-01']\n",
    "}\n",
    "\n",
    "# set season_ranges dates to datetime type\n",
    "season_start_end_dates = {season: pd.to_datetime(dates) for season, dates in season_ranges.items()}\n",
    "\n",
    "# pass in date to return season according to season_ranges\n",
    "def assign_season(date):\n",
    "    for season, (start, end) in season_start_end_dates.items():\n",
    "        if start <= date < end:\n",
    "            return season\n",
    "    return None\n",
    "\n",
    "# pass dep_arr_time into function and add returned season to season column\n",
    "final_df['season'] = final_df['dep_arr_time'].apply(assign_season)\n",
    "\n",
    "# replace Winter1 and Winter2 with Winter\n",
    "final_df['season'] = final_df['season'].replace({'Winter1': 'Winter', 'Winter2': 'Winter'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a categorical 'Time Of Day' variable to allow for delay analysis by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map time ranges for time of day categories\n",
    "time_of_day_ranges = {\n",
    "    'Night1': (0, 6),      # Night1: 00:00 - 05:59\n",
    "    'Morning': (6, 12),    # Morning: 06:00 - 11:59\n",
    "    'Afternoon': (12, 18), # Afternoon: 12:00 - 17:59\n",
    "    'Evening': (18, 21),   # Evening: 18:00 - 20:59\n",
    "    'Night2': (21, 24)     # Night2: 21:00 - 23:59\n",
    "}\n",
    "\n",
    "# pass in 'dep_arr_time' to return the time of day according to time_of_day_ranges\n",
    "def assign_time_of_day(time):\n",
    "    for period, (start_hour, end_hour) in time_of_day_ranges.items():\n",
    "        if start_hour <= time.hour < end_hour:\n",
    "            return period\n",
    "    return None  \n",
    "\n",
    "# pass 'dep_arr_time' to function, and add returned category to 'time_of_day' column\n",
    "final_df['time_of_day'] = final_df['dep_arr_time'].apply(assign_time_of_day)\n",
    "\n",
    "# replace Night1 and Night2 with Night\n",
    "final_df['time_of_day'] = final_df['time_of_day'].replace({'Night1': 'Night', 'Night2': 'Night'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Wind Speed Bins:\n",
    "The wind_speed variable is categorized into four quantile-based bins: 'Low', 'Medium', 'High', and 'Very High'. This allows for analysis of delays relative to wind speed intensity.\n",
    "\n",
    "Identify Precipitation Events:\n",
    "A new column, is_precipitation, is created to flag rows where any precipitation indicator (rain or snow in 1-hour or 3-hour intervals) is present. Rows with any non-NA values in these columns are marked as 1.\n",
    "\n",
    "Mark Significant Delays:\n",
    "The has_delay15 column is added to indicate flights delayed by 15 or more minutes. A flight is marked with 1 if either the arrival or departure delay exceeded 15 minutes; otherwise, it is marked as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 'wind_speed' bins relative to quantiles\n",
    "final_df['wind_speed_bins'] = pd.qcut(final_df['wind_speed'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# mark 1 for is_precipitation if rain or snow in indicator columns\n",
    "final_df['is_precipitation'] = final_df[['rain_1h', 'rain_3h', 'snow_1h', 'snow_3h']].apply(\n",
    "    lambda row: 1 if row.notna().any() else 0, axis=1)\n",
    "\n",
    "# mark 1 for has_delay15 if either arrival or departure was delayed by 15 or more min\n",
    "final_df['has_delay15'] = ((final_df['arr_del15'] == 1) | (final_df['dep_del15'] == 1)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a categorical 'Temperature' variable to allow for delay analysis by temperature ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map bins and associated labels for temperature catagorical\n",
    "bins = [-float('inf'), 32, 50, 75, float('inf')]\n",
    "labels = ['Very Cold', 'Cold', 'Warm', 'Hot']\n",
    "\n",
    "# create a new column based on these bins\n",
    "final_df['temp_bins'] = pd.cut(final_df['temp'], bins=bins, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle outliers in delay times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_cutoff = 3\n",
    "\n",
    "# create a mask of rows with delayed flights\n",
    "delayed_mask = final_df['has_delay15'] == 1\n",
    "\n",
    "# mean and standard deviation for arr_delay_new and dep_delay_new for delayed flights only\n",
    "arr_delay_mean = np.mean(final_df.loc[delayed_mask, 'arr_delay_new'])\n",
    "arr_delay_std = np.std(final_df.loc[delayed_mask, 'arr_delay_new'])\n",
    "dep_delay_mean = np.mean(final_df.loc[delayed_mask, 'dep_delay_new'])\n",
    "dep_delay_std = np.std(final_df.loc[delayed_mask, 'dep_delay_new'])\n",
    "\n",
    "# print mean and max values before imputation\n",
    "print(\"Before Imputation:\")\n",
    "print(f\"Mean arrival delay (arr_delay_new): {final_df['arr_delay_new'].mean():.2f}\")\n",
    "print(f\"Mean departure delay (dep_delay_new): {final_df['dep_delay_new'].mean():.2f}\")\n",
    "print(f\"Max arrival delay (arr_delay_new): {final_df['arr_delay_new'].max():.2f}\")\n",
    "print(f\"Max departure delay (dep_delay_new): {final_df['dep_delay_new'].max():.2f}\")\n",
    "\n",
    "# if the abs(z-score) of the delay time is above z_threshold, replace with NaN\n",
    "final_df.loc[delayed_mask, 'arr_delay_new'] = final_df.loc[delayed_mask, 'arr_delay_new'].apply(\n",
    "    lambda x: x if abs((x - arr_delay_mean) / arr_delay_std) < z_cutoff else np.nan)\n",
    "\n",
    "final_df.loc[delayed_mask, 'dep_delay_new'] = final_df.loc[delayed_mask, 'dep_delay_new'].apply(\n",
    "    lambda x: x if abs((x - dep_delay_mean) / dep_delay_std) < z_cutoff else np.nan)\n",
    "\n",
    "# calculate the median of delayed arrivals and departures\n",
    "arr_delay_median_delayed = final_df.loc[delayed_mask, 'arr_delay_new'].median()\n",
    "dep_delay_median_delayed = final_df.loc[delayed_mask, 'dep_delay_new'].median()\n",
    "\n",
    "# impute NaN values with the respective median for arrival or depature flights\n",
    "final_df['arr_delay_new'] = final_df['arr_delay_new'].fillna(arr_delay_median_delayed)\n",
    "final_df['dep_delay_new'] = final_df['dep_delay_new'].fillna(dep_delay_median_delayed)\n",
    "\n",
    "# print mean and max values after imputation\n",
    "print(\"\\nAfter Imputation:\")\n",
    "print(f\"Mean arrival delay (arr_delay_new): {final_df['arr_delay_new'].mean():.2f}\")\n",
    "print(f\"Mean departure delay (dep_delay_new): {final_df['dep_delay_new'].mean():.2f}\")\n",
    "print(f\"Max arrival delay (arr_delay_new): {final_df['arr_delay_new'].max():.2f}\")\n",
    "print(f\"Max departure delay (dep_delay_new): {final_df['dep_delay_new'].max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalize the Dataframe structure for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an intuitive column order\n",
    "column_order = [\n",
    "    'op_unique_carrier', 'carrier_name', 'parent_carrier_name', \n",
    "    'origin', 'dest', 'distance', 'parent_id',\n",
    "    'dep_time', 'arr_time', 'dep_arr_time', 'time_of_day',\n",
    "    'dep_delay_new', 'dep_del15', 'arr_delay_new', 'arr_del15','has_delay15',\n",
    "    'temp', 'temp_min', 'temp_max', 'wind_speed', 'wind_gust',\n",
    "    'rain_1h', 'rain_3h', 'snow_1h', 'snow_3h', 'clouds_all',\n",
    "    'is_precipitation', 'wind_speed_bins', 'temp_bins', 'season'\n",
    "]\n",
    "\n",
    "# reorder the columns in final_df\n",
    "final_df = final_df[column_order]\n",
    "\n",
    "### optional output the final-df to CSV\n",
    "\n",
    "# output_path = r'C:\\Projects\\ism645_flight_delay_analysis\\final_flight_data.csv'\n",
    "# final_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block just contains a function to render results in dataframe formats as tables\n",
    "\n",
    "def render_results_table(result_df, title):\n",
    "    \"\"\"\n",
    "    Renders a dataframe as a table in a matplotlib figure.\n",
    "\n",
    "    Args:\n",
    "        result_df (pd.DataFrame): The DataFrame containing results to render.\n",
    "        title (str): The title for the table.\n",
    "    \"\"\"\n",
    "    # create figure with hight adjusted to number of rows in dataframe\n",
    "    fig, ax = plt.subplots(figsize=(9, len(result_df) * 0.4))  # Adjust height based on the number of rows\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Create the table\n",
    "    table = ax.table(\n",
    "        cellText = result_df.values, \n",
    "        colLabels = result_df.columns,\n",
    "        rowLabels = result_df.index, \n",
    "        cellLoc='center', \n",
    "        loc='center'\n",
    "    )\n",
    "\n",
    "    # set font size and dynamically adjust column width\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.auto_set_column_width(col=list(range(len(result_df.columns))))\n",
    "    \n",
    "    # set title and show table\n",
    "    plt.title(title, fontsize=12, pad=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary Statistics\n",
    "\n",
    "This section provides an analysis of the dataset by examining summary statistics for key numerical variables. The code block below generates descriptive statistics for select numerical variables and displays them in a formatted table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the variable names for the numerical columns\n",
    "numerical_columns = ['distance', 'dep_delay_new', 'arr_delay_new', 'has_delay15', 'is_precipitation',\n",
    "                     'temp', 'wind_speed']\n",
    "\n",
    "# calculate summary statistics for the numerical variables\n",
    "summary_statistics = final_df[numerical_columns].describe().T\n",
    "\n",
    "# round the calculated values to 2 decimal places and select desired columns\n",
    "summary_statistics = summary_statistics.round(2)\n",
    "summary_statistics = summary_statistics[['mean', 'std', 'min', 'max']]\n",
    "\n",
    "render_results_table(summary_statistics, \"Summary Statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency Analysis\n",
    "\n",
    "In this section, we analyze the frequency distributions of categorical variables: `parent_carrier_name`, `wind_speed_bins`, `temp_bin`, and `season`. Each variable's frequency is calculated as a percentage of the total and visualized using bar plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_data = {\n",
    "    'parent_carrier_name': final_df['parent_carrier_name'].value_counts(normalize=True) * 100,\n",
    "    'wind_speed_bins': final_df['wind_speed_bins'].value_counts(normalize=True) * 100,\n",
    "    'temp_bins': final_df['temp_bins'].value_counts(normalize=True) * 100,\n",
    "    'season': final_df['season'].value_counts(normalize=True) * 100\n",
    "}\n",
    "\n",
    "# Create a bar plot for each variable in the frequency table\n",
    "fig, plot_axes = plt.subplots(len(frequency_data), 1, figsize=(7, 20))\n",
    "\n",
    "for plot_axis, (variable_name, frequency_table) in zip(plot_axes, frequency_data.items()):\n",
    "    frequency_table.plot(kind='bar', ax=plot_axis, color='royalblue')\n",
    "    plot_axis.set_title(f\"Frequency Distribution of {variable_name}\")\n",
    "    plot_axis.set_xlabel(variable_name)\n",
    "    plot_axis.set_ylabel(\"Delay Percentage (%)\")\n",
    "    plot_axis.set_xticklabels(plot_axis.get_xticklabels(), rotation=35, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Analysis\n",
    "\n",
    "This section presents a correlation analysis to examine the relationships between numerical variables, including flight delays, weather conditions, and distance. A heatmap visualization is used to present the results, with annotations showing the correlation coefficients for easier interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map desired numerical columns from final_df\n",
    "corr_columns = [\n",
    "    'distance', 'dep_delay_new', 'arr_delay_new', 'temp', 'temp_min', 'temp_max',\n",
    "    'wind_speed', 'wind_gust', 'clouds_all', 'is_precipitation'\n",
    "]\n",
    "\n",
    "# create df_corr using only the selected numerical variables\n",
    "df_corr = final_df[corr_columns]\n",
    "\n",
    "# calculate the correlations\n",
    "correlation_matrix = df_corr.corr()\n",
    "\n",
    "# plot the correlation heatmap\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.2)\n",
    "plt.title('Correlation Matrix of Flight and Weather Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Significance Testing\n",
    "\n",
    "Key questions:\n",
    "\n",
    "1. Are there significant differences in delay proportions between carriers, season, or time of day?\n",
    "2. Does the proportion of delays differ significantly between flights that occur during precipitation versus non-precipitation conditions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test for differences in delay times between carriers\n",
    "\n",
    "# count number of delayed and on_time flights by carrier\n",
    "carrier_num_delays = (\n",
    "    final_df.groupby('parent_carrier_name')['has_delay15']\n",
    "    .value_counts()\n",
    "    .unstack(fill_value=0)\n",
    "    .rename(columns={0: 'on_time', 1: 'delayed'})\n",
    ")\n",
    "\n",
    "# calculate expected values\n",
    "row_sums = carrier_num_delays.sum(axis=1)\n",
    "col_sums = carrier_num_delays.sum(axis=0)\n",
    "total = row_sums.sum()\n",
    "expected = np.outer(row_sums, col_sums) / total\n",
    "expected = expected[:, 0:2]\n",
    "\n",
    "# set up a dataframe for the results\n",
    "result = pd.DataFrame({\n",
    "    'On Time': carrier_num_delays['on_time'],\n",
    "    'Delayed': carrier_num_delays['delayed'],\n",
    "    'Proportion': (carrier_num_delays['delayed'] / (carrier_num_delays['on_time'] + carrier_num_delays['delayed'])).round(2),\n",
    "    'Expected On Time': expected[:, 0].round(2),\n",
    "    'Expected Delayed': expected[:, 1].round(2)\n",
    "}, index=carrier_num_delays.index)\n",
    "\n",
    "\n",
    "# calculate chi2 for each airline and p-value using the observed and expected values\n",
    "\n",
    "# empty lists to store values\n",
    "chi2_values = []\n",
    "p_values = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    # observed values for each airline\n",
    "    observed = carrier_num_delays.iloc[i].values\n",
    "    expected_vals = expected[i]\n",
    "    \n",
    "    # calculate chi2 statistic for each airline\n",
    "    chi2_stat = np.sum((observed - expected_vals)**2 / expected_vals)\n",
    "    \n",
    "    # compute the p-value for the chi2 statistic (df=1 for 2x2 table)\n",
    "    p_val = 1 - stats.chi2.cdf(chi2_stat, df=1)\n",
    "    \n",
    "    # append values to lists\n",
    "    chi2_values.append(round(chi2_stat, 2))\n",
    "    p_values.append(round(p_val, 4))\n",
    "\n",
    "# add chi2 and p-value columns to the result dataframe\n",
    "result['Chi2'] = chi2_values\n",
    "result['P-Value'] = p_values\n",
    "\n",
    "render_results_table(result, \"ChiSquare Test for Significant Delay Proportion Differences Between Carriers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many of the variables, it makes sense to test whether or not the proportions within each category (i.e. Morning, Afternoon etc in time_of_day) are statistically different from the overall proportion of delays. To enable processing of any desired categorical column, a function is defined below to run the z-test for proportions and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests for statistically significant difference in proportion of delays for unique categories from provided column against overall frequency of has_delay15\n",
    "\n",
    "def test_category_deviation_from_overall(categorical_column):\n",
    "\n",
    "    #calculate the proportion of delays for all flights in 2023 at RDU\n",
    "    overall_prop = final_df['has_delay15'].mean()\n",
    "\n",
    "    # initialize a list to store results\n",
    "    results = []\n",
    "\n",
    "    # loop through each unique category within the passed in categorical_column (i.e. Spring, Summer....)\n",
    "    for category, group in final_df.groupby(categorical_column):\n",
    "        # delay proportion for the provided category\n",
    "        category_prop = group['has_delay15'].mean()\n",
    "        # number of observations in current category\n",
    "        num_obs = group.shape[0]\n",
    "\n",
    "        # z-score calculation for category delay proportion\n",
    "        standard_error = (overall_prop * (1 - overall_prop) / num_obs) ** 0.5\n",
    "        z = (category_prop - overall_prop) / standard_error\n",
    "\n",
    "        # p-value for the z-test\n",
    "        p_value = 2 * (1 - stats.norm.cdf(abs(z)))  # Two-tailed test\n",
    "\n",
    "        # append to results list\n",
    "        results.append({\n",
    "            f'{categorical_column}': category,\n",
    "            f'Proportion': round(category_prop, 2),\n",
    "            'Overall Proportion': round(overall_prop, 2),\n",
    "            'Z-Score': round(z, 2),\n",
    "            'P-Value': round(p_value, 4)\n",
    "        })\n",
    "\n",
    "    # convert results to dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# some examples below - other categories include wind_speed_bins, temp_bin, & time_of_day\n",
    "results_airlines = test_category_deviation_from_overall('parent_carrier_name')\n",
    "render_results_table(results_airlines, \"Carrier vs Overall Delay Proportion\")\n",
    "\n",
    "results_seasons = test_category_deviation_from_overall('season')\n",
    "render_results_table(results_seasons, \"Season vs Overall Delay Proportion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this codeblock is essentailly the same as the test function but shows the steps a little more clearly\n",
    "# you can confirm the similarity by looking at the output of passing the is_precipitation column into the test_category_deviation_from_overall function (in the next codeblock)\n",
    "\n",
    "# Group 1: Precipitation = 1\n",
    "delays_precip = final_df[(final_df['is_precipitation'] == 1) & (final_df['has_delay15'] == 1)].shape[0]  # Delays with precipitation\n",
    "total_precip = final_df[final_df['is_precipitation'] == 1].shape[0]  # Total flights with precipitation\n",
    "\n",
    "# Group 2: Precipitation = 0\n",
    "delays_no_precip = final_df[(final_df['is_precipitation'] == 0) & (final_df['has_delay15'] == 1)].shape[0]  # Delays without precipitation\n",
    "total_no_precip = final_df[final_df['is_precipitation'] == 0].shape[0]  # Total flights without precipitation\n",
    "\n",
    "\n",
    "# calculate the with/without precipitation proportions\n",
    "p_precip = delays_precip / total_precip\n",
    "p_no_precip = delays_no_precip / total_no_precip\n",
    "\n",
    "# calculate the pooled proportion\n",
    "total_delays = delays_precip + delays_no_precip\n",
    "total_flights = total_precip + total_no_precip\n",
    "pooled_prop = total_delays / total_flights\n",
    "\n",
    "\n",
    "standard_error = np.sqrt(pooled_prop * (1 - pooled_prop) * (1 / total_precip + 1 / total_no_precip))\n",
    "z_stat = (p_precip - p_no_precip) / standard_error\n",
    "\n",
    "# perform z-test\n",
    "p_value = 1 - stats.norm.cdf(z_stat)\n",
    "\n",
    "# display the results\n",
    "print(f'Proportion with precipitation: {p_precip:.3f}')\n",
    "print(f'Proportion without precipitation: {p_no_precip:.3f}')\n",
    "print(f'Z-statistic: {z_stat:.2f}')\n",
    "print(f'P-value: {p_value:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison to the above codeblock using the functions to calculate and render the results\n",
    "\n",
    "render_results_table(test_category_deviation_from_overall('is_precipitation'), \"Is Precipitation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
